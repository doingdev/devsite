import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

def extract_title_and_desc(soup):
    title = (soup.title.string.strip() if soup.title and soup.title.string else "")
    desc = ""
    meta = soup.find('meta', attrs={'name': 'description'}) or soup.find('meta', attrs={'property':'og:description'})
    if meta and meta.get('content'):
        desc = meta['content'].strip()
    # Fallback: first H1 or short P
    if not desc and soup.h1 and soup.h1.text.strip():
        desc = soup.h1.text.strip()
    elif not desc:
        # First non-empty <p>
        for p in soup.find_all('p'):
            t = p.text.strip()
            if len(t) > 20:
                desc = t
                break
    return title, desc

def find_internal_links(soup, base_url, max_links=5):
    domain = urlparse(base_url).netloc
    links = []
    for a in soup.find_all('a', href=True):
        href = urljoin(base_url, a['href'])
        # Internal only, skip fragments/mailto/etc.
        netloc = urlparse(href).netloc
        if netloc == domain and href != base_url and not href.startswith("mailto") and not href.endswith(".jpg"):
            text = a.get_text(strip=True) or "Untitled"
            if href not in [l['url'] for l in links] and len(links) < max_links:
                links.append({'title': text, 'url': href})
    return links

def extract_contact(soup):
    # Email or "Contact" link
    # Email pattern
    emails = set(re.findall(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+", soup.get_text()))
    if emails:
        return min(emails, key=len)
    # Look for contact page
    for a in soup.find_all("a", href=True):
        if 'contact' in (a.text.lower() + a['href'].lower()):
            return urljoin('https://'+urlparse(a['href']).netloc, a['href'])
    return ""

def crawl_and_extract_llms_elements(url, crawl_links=3):
    try:
        r = requests.get(url, timeout=10, headers={"User-Agent":"Mozilla/5.0"})
        soup = BeautifulSoup(r.text, "html.parser")
    except Exception as ex:
        return None

    title, desc = extract_title_and_desc(soup)
    main_links = find_internal_links(soup, url, max_links=crawl_links)
    contact = extract_contact(soup)

    # — Optionally fetch the title/desc of up to crawl_links internal pages
    resource_links = []
    for l in main_links:
        try:
            sub_r = requests.get(l['url'], timeout=6, headers={"User-Agent":"Mozilla/5.0"})
            sub_soup = BeautifulSoup(sub_r.text, "html.parser")
            sub_title, sub_desc = extract_title_and_desc(sub_soup)
            resource_links.append({
                'title': sub_title or l['title'],
                'url': l['url'],
                'desc': sub_desc or ''
            })
        except Exception:
            continue

    # Output dict for llms.txt generator
    result = {
        "title": title,
        "description": desc,
        "resources": resource_links,
        "contact": contact
    }
    return result

def elements_to_llms_txt(elements):
    if not elements:
        return "# (error)\n> No content extracted.\n"
    lines = []
    # Title (H1)
    lines.append(f"# {elements['title'] or 'Untitled Site'}")
    # Blockquote summary
    if elements.get('description'):
        lines.append(f"> {elements['description']}")
    lines.append("")
    # Main Resources section
    if elements.get('resources'):
        lines.append("## Core Content")
        for r in elements['resources']:
            # Safe formatting
            title = r.get('title', '').replace("\n", " ").strip()
            url = r.get('url', '').strip()
            desc = r.get('desc', '').replace("\n", " ").strip()
            if url:
                lines.append(f"- [{title}]({url}): {desc}")
    # Contact info
    if elements.get('contact'):
        lines.append(f"\nContact: {elements['contact']}")
    lines.append("\nGenerated by llms.txt generator @doingdev")
    return "\n".join(lines)


# NEED UNit Test
